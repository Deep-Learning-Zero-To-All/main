{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST?\n",
    "- handwritten digits dataset\n",
    "- 손으로 쓴 숫자 데이터 세트.\n",
    "- view 함수를 사용해서 숫자로 바꿈.\n",
    "- `torchvision` 이라는 패키지를 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 이걸.. 쿠바로 잘못써서,, 몇분이 날아갔다..\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=dsets.MNIST(root=\"MNIST_data/\", \n",
    "                        train=True, \n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=True)\n",
    "mnist_test=dsets.MNIST(root=\"MNIST_data/\",\n",
    "                       train=False, \n",
    "                       transform=transforms.ToTensor(), \n",
    "                       download=True)\n",
    "# train=true>> train data train=False>>test data\n",
    "# ToTensor 이미지형 데이터를 파이토치형 데이터로 바꿔주는 역할\n",
    "\n",
    "data_loader=torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "#중간에 data.을 추가해야 된다!\n",
    "# 그리고 첫번째 인자 dataloader가 아니라 dataset이다!\n",
    "# batch_size 몇개씩 불러올래? shuffle 무작위로 부를래? 순서대로 부를래? \n",
    "# drop last 배치사이즈에 맞지 않게 남는 데이터를 자를까?\n",
    "\n",
    "linear=torch.nn.Linear(784,10,bias=True).to(device)\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer=torch.optim.SGD(linear.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.531608760\n",
      "Epoch: 0002 cost= 0.359284073\n",
      "Epoch: 0003 cost= 0.331268132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-adf490bc3aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mavg_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=len(data_loader)\n",
    "    for X,Y in data_loader:\n",
    "        X=X.view(-1,28*28).to(device)\n",
    "        Y=Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis=linear(X)\n",
    "        cost=criterion(hypothesis,Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost+=cost/total_batch\n",
    "        \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잠시 epoch, batch_size, iteration 설명~\n",
    "1000개 data batch_size 500이면 2번의 epoch 후에 1번의 iteration이 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 0.537019730\n",
      "Epoch:  0002 cost = 0.359070480\n",
      "Epoch:  0003 cost = 0.331466913\n",
      "Epoch:  0004 cost = 0.316397995\n",
      "Epoch:  0005 cost = 0.307214856\n",
      "Epoch:  0006 cost = 0.300222486\n",
      "Epoch:  0007 cost = 0.295077562\n",
      "Epoch:  0008 cost = 0.290781766\n",
      "Epoch:  0009 cost = 0.287452102\n",
      "Epoch:  0010 cost = 0.284511983\n",
      "Epoch:  0011 cost = 0.281673819\n",
      "Epoch:  0012 cost = 0.279699981\n",
      "Epoch:  0013 cost = 0.277923882\n",
      "Epoch:  0014 cost = 0.276094288\n",
      "Epoch:  0015 cost = 0.274461269\n"
     ]
    }
   ],
   "source": [
    "linear=torch.nn.Linear(784,10,bias=True).to(device)\n",
    "# output =10인 이유는 이미지 숫자는 0~9까지 있음.\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer=torch.optim.SGD(linear.parameters(),lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=len(data_loader)\n",
    "    for X,Y in data_loader:\n",
    "        X=X.view(-1,28*28).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis=linear(X)\n",
    "        cost=criterion(hypothesis,Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost+=cost/total_batch\n",
    "        \n",
    "    print(\"Epoch: \",\"%04d\"%(epoch+1),\"cost =\",\"{:.9f}\".format(avg_cost))\n",
    "    #11:27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8798999786376953\n",
      "Label:  5\n",
      "Prediction:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 만든 모델을 테스트 셋을 이용해서 테스트해보자.\n",
    "# no_grad 이 범위안에서는 gradient를 계산 안하겠다.\n",
    "with torch.no_grad():\n",
    "    X_test=mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test=mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction=linear(X_test)\n",
    "    #불러온 set을 linear 모델에 넣으면 예측값을 얻을 수 있다.\n",
    "    correct_prediction=torch.argmax(prediction,1)==Y_test\n",
    "    #예측과 실제 test 레이블 간에 일치하는지 계산\n",
    "    accuracy=correct_prediction.float().mean()\n",
    "    # 일치하는지에 대한 평균을 구하면 정확도를 알 수 있음\n",
    "    print('Accuracy: ',accuracy.item())\n",
    "    \n",
    "    r=random.randint(0,len(mnist_test)-1)\n",
    "    #test data set 중에 무작위로 이미지를 하나 뽑아서 학습한 모델에 넣는다,\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ',Y_single_data.item())\n",
    "    single_prediction=linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction,1).item())\n",
    "    \n",
    "    #예측해본 이미지를 matplotlib를 이용해서 띄운다\n",
    "    plt.imshow(mnist_test.test_data[r:r+1].view(28,28),cmap='Greys',interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

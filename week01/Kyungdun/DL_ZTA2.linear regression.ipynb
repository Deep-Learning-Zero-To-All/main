{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- y=Wx+b 의 꼴로 나타낸다.\n",
    "- W는 weight, b는 bias라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=torch.zeros(1,requires_grad=True) #requires_grad=True 학습할 것이라고 명시\n",
    "b=torch.zeros(1,requires_grad=True) # weight 와 bias를 0으로 초기화, 항상 출력 0을 예측.\n",
    "hypothesis=x_train*W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 계산\n",
    "- MSE(Mean Squared Error) 를 이용해서 loss 계산한다.\n",
    "- 예측값과 진짜 y 사이 차이의 제곱을 평균낸 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=torch.mean((hypothesis-y_train)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 계산한 loss를 이용해서 모델을 개선\n",
    "- SGD(Stochastic Gradient Descent) 기법을 이용하자.\n",
    "    - torch.optim 라이브러리를 사용한다.\n",
    "    - [W,b]는 학습할 텐서\n",
    "    - lr=0.01은 learning rate\n",
    "    \n",
    "    \n",
    "- 항상 붙어다니는 세 줄\n",
    "    - zero_grad() gradient 초기화\n",
    "    - backward() gradient 계산\n",
    "    - step() 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD([W,b],lr=0.01) #띠요오옹 나와있는거 다 합쳤더니 됐다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기까지는 한 번만 진행!\n",
    "1. 데이터 정의\n",
    "2. Hypothesis 초기화\n",
    "3. Optimizer 정의\n",
    "\n",
    "#### 아래부터는 반복 진행!\n",
    "1. Hypothesis 예측\n",
    "2. Cost 계산\n",
    "3. Optimizer로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs=1000\n",
    "for epoch in range(1,nb_epochs+1):\n",
    "    hypothesis=x_train*W + b\n",
    "    cost=torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2.0000], requires_grad=True), tensor([8.7018e-06], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print([W,b]) # 학습 횟수에 따라 값이 다르게 나온다. 신기!                                                                                                                                                                                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- Hypothesis function 복습\n",
    "- 사용할 모의 data 확인\n",
    "- Cost function 이해\n",
    "- Gradient descent 이론 \n",
    "- Gradient descent 구현\n",
    "- 구현 (nn.optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis function 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "W=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "hypothesis = x_train*W + b\n",
    "\n",
    "# 이번 수업에서는 b 없이 W*x_train 으로 나타내보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function\n",
    "- 모델 예측값이 실제 데이터와 얼마나 다른지 나타내는 값.\n",
    "- 잘 학습될수록 낮은 값을 가진다.\n",
    "- 보통 MSE를 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient descent\n",
    "- cost를 w로 미분한 값을 gradient라고 한다.\n",
    "    - cost는 w에 대한 2차 함수이므로 gradient는 간단한 미분으로 계산할 수 있다.\n",
    "    - W = W - a*gradient 여기서 a는 Learning rate > 통상적으로 lr로 줄여 쓴다.\n",
    "    - 이 과정을 gradient descent라고 한다. gradient를 이용해서 cost를 줄인다는 뜻이다.\n",
    "- 기울기가 양수> W 작아져야 함.\n",
    "- 기울기가 음수> W 커져야 함(0으로 가야 함)\n",
    "- 기울기가 가파름> 많이 변해야 함.\n",
    "- 기울기가 완만함> 조금 변해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient=2*torch.mean((W*x_train-y_train)*x_train) # (W*x_train-y_train)**2 를 W로 미분한 식\n",
    "# lr=0.1\n",
    "# W-= lr * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400, Cost: 0.746666\n",
      "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Epoch: 데이터로 학습한 횟수.\n",
    "# 학습하면서 점점 W는 1에 수렴하고 cost가 줄어든다.\n",
    "\n",
    "#데이터\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "#모델 초기화\n",
    "W=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "#Learning rate 설정\n",
    "\n",
    "\n",
    "np_epochs=10\n",
    "for epoch in range(np_epochs+1):\n",
    "    \n",
    "    #H(x) 계산하기\n",
    "    hypothesis = x_train * W\n",
    "    lr=0.1\n",
    "    #cost gradient 계산\n",
    "    cost=torch.mean((hypothesis-y_train)**2)\n",
    "    gradient=torch.sum((W*x_train-y_train)*x_train)\n",
    "    \n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
    "    epoch , np_epochs , W.item(), cost.item()\n",
    "    ))\n",
    "    \n",
    "    # cost gradient로 H(x) 계산\n",
    "    W =W- lr*gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.optim으로 gradient descent 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fbdc6ebccb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# optimizer 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#cost로 H(x) 개선\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer=torch.optim.SGD([W],lr=0.15)\n",
    "\n",
    "#cost로 H(x) 개선\n",
    "optimizer.zero_grad() # gradient 초기화\n",
    "cost.backward() #gradient 계산\n",
    "optimizer.step() # gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400, Cost: 0.746667\n",
      "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#데이터\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "#모델 초기화\n",
    "W=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer=torch.optim.SGD([W],lr=0.15)\n",
    "\n",
    "nb_epochs=10\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis=x_train*W\n",
    "    \n",
    "    cost=torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
    "    epoch , np_epochs , W.item(), cost.item()\n",
    "    ))\n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() #gradient 계산\n",
    "    optimizer.step() # gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 시간: 여러 개의 모델로 결론을 추측하는 모델을 만들자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
